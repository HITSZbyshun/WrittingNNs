{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:25.634240Z",
     "start_time": "2025-01-12T13:20:25.623331Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 14,
   "source": [
    "## 导包和初始化\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import Image\n",
    "# default: 100\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "torch.manual_seed(42)"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 1. 编码器"
   ],
   "id": "6354e692c3972bfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## 初始化参数\n",
    "d_model = 4  # 模型维度\n",
    "nhead = 2    # 多头注意力中的头数\n",
    "dim_feedforward = 8  # 前馈网络的维度\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "\n",
    "## 保证多头可以整除\n",
    "assert d_model % nhead == 0"
   ],
   "id": "38ff54e12cd3082a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:25.682457Z",
     "start_time": "2025-01-12T13:20:25.678462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 配置输入\n",
    "encoder_input = torch.randn(seq_len, batch_size, d_model) \n",
    "print(encoder_input)\n",
    "print(encoder_input.shape)"
   ],
   "id": "24804b85a53d85c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3367,  0.1288,  0.2345,  0.2303]],\n",
      "\n",
      "        [[-1.1229, -0.1863,  2.2082, -0.6380]],\n",
      "\n",
      "        [[ 0.4617,  0.2674,  0.5349,  0.8094]]])\n",
      "torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:25.800043Z",
     "start_time": "2025-01-12T13:20:25.789117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 利用已封装的编码器类实现推理\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                           dim_feedforward=dim_feedforward, dropout=0.0)\n",
    "memory = encoder_layer(encoder_input)  # 编码器输出\n",
    "\n",
    "print(memory)\n",
    "print(memory.shape)"
   ],
   "id": "fbfcd339edf548d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0328, -0.9185,  0.6710,  1.2804]],\n",
      "\n",
      "        [[-1.4175, -0.1948,  1.3775,  0.2347]],\n",
      "\n",
      "        [[-1.0022, -0.8035,  0.3029,  1.5028]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:25.851826Z",
     "start_time": "2025-01-12T13:20:25.844707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 手写编码器——1.实现多头注意力机制\n",
    "### 1.1. 展平输入\n",
    "X = encoder_input\n",
    "X_flat = X.contiguous().view(-1, d_model)  # [T * B, d_model] -> [3, 4]\n",
    "X_flat.shape\n",
    "\n",
    "### 1.2. 获取权重\n",
    "self_attn = encoder_layer.self_attn\n",
    "W_in = self_attn.in_proj_weight\n",
    "b_in = self_attn.in_proj_bias\n",
    "W_out = self_attn.out_proj.weight\n",
    "b_out = self_attn.out_proj.bias\n",
    "\n",
    "### 1.3. 计算Q、K、V\n",
    "QKV = F.linear(X_flat, W_in, b_in)  # [3, 3*d_model]\n",
    "Q, K, V = QKV.split(d_model, dim=1)  # 每个维度为[3, d_model], 实际就是[TxB，d_model]\n",
    "\n",
    "### 1.4. 分开为多头注意力\n",
    "head_dim = d_model // nhead  # 每个头的维度\n",
    "def reshape_for_heads(x):\n",
    "    return x.contiguous().view(seq_len, batch_size, nhead, head_dim).permute(1, 2, 0, 3).reshape(batch_size * nhead, seq_len, head_dim)\n",
    "Q = reshape_for_heads(Q)\n",
    "K = reshape_for_heads(K)\n",
    "V = reshape_for_heads(V)\n",
    "\n",
    "### 1.5. 计算注意力分数\n",
    "scores = torch.bmm(Q, K.transpose(1, 2)) / (head_dim ** 0.5)  # [batch_size * nhead, seq_len, seq_len]\n",
    "attn_weights = F.softmax(scores, dim=-1)  # [batch_size * nhead, seq_len, seq_len]  应用softmax\n",
    "\n",
    "### 1.6. 计算注意力输值\n",
    "attn_output = torch.bmm(attn_weights, V)  # [batch_size * nhead, seq_len, head_dim]\n",
    "\n",
    "### 1.7. 合并多头\n",
    "attn_output = attn_output.view(batch_size, nhead, seq_len, head_dim).permute(2, 0, 1, 3).contiguous()\n",
    "attn_output = attn_output.view(seq_len, batch_size, d_model)  # [seq_len, batch_size, d_model]\n",
    "\n",
    "### 1.8. 输出投影\n",
    "attn_output = F.linear(attn_output.view(-1, d_model), W_out, b_out)  # [seq_len * batch_size, d_model]\n",
    "attn_output = attn_output.view(seq_len, batch_size, d_model)"
   ],
   "id": "f1f1766ad7f8060",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:25.910625Z",
     "start_time": "2025-01-12T13:20:25.903417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 手写编码器——2.残差连接和层归一化\n",
    "### 2.1. 获取层归一化权重和偏置\n",
    "norm1 = encoder_layer.norm1\n",
    "print(norm1.weight,norm1.bias)\n",
    "\n",
    "### 2.2. 残差连接\n",
    "residual = X + attn_output  # [seq_len, batch_size, d_model]\n",
    "normalized = F.layer_norm(residual, (d_model,), weight=norm1.weight, bias=norm1.bias)  # [seq_len, batch_size, d_model]\n",
    "print(normalized) # 实际上就是最后一维，某个batch和序列的某个token的特征向量归一化了"
   ],
   "id": "b2026df327913a21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True) Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n",
      "tensor([[[-0.9493, -1.0434,  1.1045,  0.8881]],\n",
      "\n",
      "        [[-1.0025, -0.1531,  1.6511, -0.4955]],\n",
      "\n",
      "        [[-1.0129, -0.9286,  0.6342,  1.3073]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:25.962327Z",
     "start_time": "2025-01-12T13:20:25.957793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 手写编码器——3.实现前馈网络\n",
    "### 3.1. 获取前馈网络权重和偏置\n",
    "W_1 = encoder_layer.linear1.weight\n",
    "b_1 = encoder_layer.linear1.bias\n",
    "W_2 = encoder_layer.linear2.weight\n",
    "b_2 = encoder_layer.linear2.bias\n",
    "\n",
    "### 3.2. 第一层线性变换\n",
    "ffn_output = F.linear(normalized.view(-1, d_model), W_1, b_1)  # [seq_len * batch_size, dim_feedforward]\n",
    "ffn_output = F.relu(ffn_output)  # [seq_len * batch_size, dim_feedforward]\n",
    "\n",
    "### 3.3. 第二层线性变换\n",
    "ffn_output = F.linear(ffn_output, W_2, b_2)  # [seq_len * batch_size, d_model]\n",
    "ffn_output = ffn_output.view(seq_len, batch_size, d_model)  # [seq_len, batch_size, d_model]"
   ],
   "id": "c09ba716bc028858",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:26.019822Z",
     "start_time": "2025-01-12T13:20:26.013733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 手写编码器——4.残差连接和层归一化（第二层）\n",
    "### 4.1. 获取层归一化权重和偏置\n",
    "norm2 = encoder_layer.norm2\n",
    "print(norm2.weight,norm2.bias)\n",
    "\n",
    "### 4.2. 残差连接和层归一化\n",
    "residual2 = normalized + ffn_output  # [seq_len, batch_size, d_model]\n",
    "normalized2 = F.layer_norm(residual2, (d_model,), weight=norm2.weight, bias=norm2.bias)  # [seq_len, batch_size, d_model]"
   ],
   "id": "b7714c87b7ca3e75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1.], requires_grad=True) Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:20:45.575928Z",
     "start_time": "2025-01-12T13:20:45.568216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 打印最终输出：结果一致\n",
    "### 手写编码器输出\n",
    "print(normalized2)\n",
    "### 封装编码器输出\n",
    "print(memory)"
   ],
   "id": "33638d513c7a8f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0328, -0.9185,  0.6710,  1.2804]],\n",
      "\n",
      "        [[-1.4175, -0.1948,  1.3775,  0.2347]],\n",
      "\n",
      "        [[-1.0022, -0.8035,  0.3029,  1.5028]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-1.0328, -0.9185,  0.6710,  1.2804]],\n",
      "\n",
      "        [[-1.4175, -0.1948,  1.3775,  0.2347]],\n",
      "\n",
      "        [[-1.0022, -0.8035,  0.3029,  1.5028]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 2. 解码器"
   ],
   "id": "aabbf451a3b1faab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:24:58.284064Z",
     "start_time": "2025-01-12T13:24:58.280097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 初始化参数\n",
    "# 定义参数\n",
    "d_model = 4  # 模型维度\n",
    "nhead = 2    # 多头注意力中的头数\n",
    "dim_feedforward = 8  # 前馈网络的维度\n",
    "batch_size = 1\n",
    "\n",
    "src_seq_len = 3\n",
    "trg_seq_len = 5"
   ],
   "id": "577020a975ab0724",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:24:59.111793Z",
     "start_time": "2025-01-12T13:24:59.106220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造输入\n",
    "# [T, B, d]\n",
    "encoder_input = torch.randn(src_seq_len, batch_size, d_model)  \n",
    "decoder_input = torch.randn(trg_seq_len, batch_size, d_model)"
   ],
   "id": "a5ca2b446b9aabd7",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T13:25:24.341689Z",
     "start_time": "2025-01-12T13:25:24.331609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 利用已封装的解码器类实现推理\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, \n",
    "                                           nhead=nhead,\n",
    "                                           dim_feedforward=dim_feedforward, \n",
    "                                           dropout=0.0)\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, \n",
    "                                           nhead=nhead,\n",
    "                                           dim_feedforward=dim_feedforward, \n",
    "                                           dropout=0.0)\n",
    "memory = encoder_layer(encoder_input)  # 编码器输出\n",
    "output = decoder_layer(decoder_input, memory)  # 解码器输出\n",
    "print(output)\n",
    "print(encoder_input.shape, memory.shape, output.shape)"
   ],
   "id": "670dafc9a54d6425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1402, -0.7298, -0.8038,  1.6738]],\n",
      "\n",
      "        [[-0.7154,  1.6404, -0.8922, -0.0327]],\n",
      "\n",
      "        [[ 0.0890,  0.6641, -1.6547,  0.9016]],\n",
      "\n",
      "        [[ 0.2811, -1.0920,  1.5008, -0.6900]],\n",
      "\n",
      "        [[-0.4675,  1.5222, -1.2012,  0.1465]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([3, 1, 4]) torch.Size([3, 1, 4]) torch.Size([5, 1, 4])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## 手写解码器——1.实现多头注意力机制\n",
   "id": "ad6bdea18de827ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
