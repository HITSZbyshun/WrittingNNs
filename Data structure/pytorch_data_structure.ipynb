{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec9753d",
   "metadata": {},
   "source": [
    "感谢：\n",
    "BV1Ab421J7jY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39895b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.2266, device='cuda:0', dtype=torch.float16)\n",
      "tensor(9.2266, device='cuda:0', dtype=torch.float16)\n",
      "tensor(9.2300, device='cuda:0')\n",
      "tensor(9.2301, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "fp16_1 = torch.tensor(9.2300, dtype=torch.float16).cuda()\n",
    "print(fp16_1)\n",
    "fp16_2 = torch.tensor(9.2301, dtype=torch.float16).cuda()\n",
    "print(fp16_2)\n",
    "\n",
    "\n",
    "fp32_1 = torch.tensor(9.2300, dtype=torch.float32).cuda()\n",
    "print(fp32_1)\n",
    "fp32_2 = torch.tensor(9.2301, dtype=torch.float32).cuda()\n",
    "print(fp32_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(65504., device='cuda:0', dtype=torch.float16)\n",
      "tensor(65504., device='cuda:0', dtype=torch.float16)\n",
      "tensor(inf, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "#上溢出\n",
    "fp16_up = torch.tensor(2**15+1023*2**5, dtype=torch.float16).cuda()\n",
    "print(fp16_up)\n",
    "\n",
    "fp16_up_over_1 = torch.tensor(65505, dtype=torch.float16).cuda()\n",
    "print(fp16_up_over_1)\n",
    "\n",
    "fp16_up_over_2 = torch.tensor(65520, dtype=torch.float16).cuda()\n",
    "print(fp16_up_over_2)\n",
    "\n",
    "'''\n",
    "65520 是float16 从65504跳到inf的临界值,具体推导看md文件\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b364608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9605e-08, device='cuda:0', dtype=torch.float16)\n",
      "tensor(0., device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# 下溢出\n",
    "\n",
    "'''\n",
    "当指数位为0时,计算公式发生变化\n",
    "公式为 2**(-14)*(0+x/1024)\n",
    "临界值在x为0.5时，这里不是四舍五入，而是x取0.5时，就直接舍去为0\n",
    "'''\n",
    "fp16_down = torch.tensor(2**(-24)*0.501, dtype=torch.float16).cuda()\n",
    "print(fp16_down)\n",
    "\n",
    "fp16_down_over_1 = torch.tensor(2**(-25), dtype=torch.float16).cuda()\n",
    "print(fp16_down_over_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adbd69d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='cuda:0', dtype=torch.float16)\n",
      "tensor(19984., device='cuda:0', dtype=torch.float16) tensor(20000., device='cuda:0', dtype=torch.float16)\n",
      "tensor(1.1921e-07, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "#Nan\n",
    "## softmax 函数\n",
    "m1=torch.tensor(19990, dtype=torch.float16).cuda()\n",
    "m2=torch.tensor(20000, dtype=torch.float16).cuda()\n",
    "print(torch.exp(m1)/(torch.exp(m1)+torch.exp(m2)))\n",
    "print(m1,m2)\n",
    "## 解决Nan\n",
    "\n",
    "print(torch.exp(m1-max(m1,m2))/(torch.exp(m1-max(m1,m2))+torch.exp(m2-max(m1,m2))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb8e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1000., device='cuda:0', dtype=torch.float16)\n",
      "tensor(1000., device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Nan2\n",
    "s1=torch.tensor(50*[1000], dtype=torch.float16).cuda()\n",
    "print(torch.mean(s1))\n",
    "\n",
    "# 这个在torch 2.3.1 里不会有问题，已经优化了\n",
    "# 我估摸着低版本torch会有问题\n",
    "# 这里的问题是指，batchsize过大，会优先相加分子，导致过大溢出。\n",
    "# 按理说大于 65504就会变成inf,但是torch的实现是先相加分子，然后再除以分母\n",
    "# 所以猜测这里用了迭代法计算均值\n",
    "# 但似乎torch调用的是cpp的底层mean实现，所以也许与cpp版本有关\n",
    "s2=torch.tensor(500000*[1000], dtype=torch.float16).cuda()\n",
    "print(torch.mean(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1406, dtype=torch.float16)\n",
      "tensor(3.1410)\n",
      "tensor(3.1406)\n",
      "tensor(3.1406, dtype=torch.float16)\n",
      "tensor(3.1415)\n",
      "tensor(3.1411)\n"
     ]
    }
   ],
   "source": [
    "## 数值精度\n",
    "\n",
    "import torch\n",
    "\n",
    "pi_16=torch.tensor(3.141, dtype=torch.float16)\n",
    "pi_32=torch.tensor(3.141, dtype=torch.float32)\n",
    "\n",
    "print(pi_16)\n",
    "print(pi_32)\n",
    "# 3.141 float16转float32表示\n",
    "print(pi_16.float())\n",
    "\n",
    "\n",
    "s=torch.tensor(0.0005, dtype=torch.float16)\n",
    "print(s+pi_16) # 加了等于没加\n",
    "print(s+pi_32)\n",
    "print(s+pi_16.float())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningHF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
